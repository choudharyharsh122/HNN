{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"ml tqdm/4.66.2-GCCcore-13.2.0\")\n",
    "from tqdm import tqdm\n",
    "os.system(\"ml PyTorch/2.2.1-foss-2023b-CUDA-12.4.0\")\n",
    "import torch\n",
    "import torch.nn as nn   \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import optuna\n",
    "import csv\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "import argparse\n",
    "import distutils.util\n",
    "import tracemalloc\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s', handlers=[logging.StreamHandler(sys.stdout)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HamiltonianNN(nn.Module):\n",
    "\n",
    "    def __init__(self, model_specs):\n",
    "        super(HamiltonianNN, self).__init__()\n",
    "\n",
    "        # Create a list of linear layers based on layer_sizes\n",
    "        layer_sizes = model_specs[0]\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout_layers = nn.ModuleList()\n",
    "        self.RANDOM_SEED = 0\n",
    "        for i in range(len(layer_sizes) - 2):  # All layers except the last one\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=True))\n",
    "\n",
    "            self.dropout_layers.append(nn.Dropout(p=0.2))\n",
    "        \n",
    "        # Last layer without bias\n",
    "        self.layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1], bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = layer(x)\n",
    "            x = torch.tanh(x)\n",
    "            if i < len(self.dropout_layers):\n",
    "                x = self.dropout_layers[i](x)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "    \n",
    "    def _apply_xavier_init(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.zeros_(layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_ode(y_tensor, args, kargs):\n",
    "\n",
    "    model = args[0]\n",
    "    i = kargs[0]\n",
    "        \n",
    "    with torch.enable_grad():\n",
    "\n",
    "        #y = y_tensor.clone().detach().requires_grad_(True)\n",
    "\n",
    "        y = y_tensor.requires_grad_(True)\n",
    "\n",
    "        h = model(y)\n",
    "\n",
    "    \n",
    "        grad_h = torch.autograd.grad(outputs=h.sum(), inputs=y, create_graph=True, allow_unused=True)[0]\n",
    "        #print(\"grad h: \", grad_h)\n",
    "        dq_dt = grad_h[:, 1]\n",
    "        dp_dt = -grad_h[:, 0]\n",
    "\n",
    "    return torch.stack((dq_dt, dp_dt), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjoint_ode(lam, args, kargs):\n",
    "\n",
    "    (model, y_values) = args\n",
    "    batch_size = y_values.shape[0]\n",
    "    (i,) = kargs\n",
    "    br_i = y_values.shape[2]//2\n",
    "\n",
    "    y_tensor = y_values[:,i,:].clone().detach().requires_grad_(True)\n",
    "    \n",
    "    h = model(y_tensor)\n",
    "\n",
    "    # Compute first-order derivatives ∇H = [∂H/∂q, ∂H/∂p]\n",
    "    grad_h = torch.autograd.grad(outputs=h.sum(), inputs=y_tensor, create_graph=True, retain_graph=True)[0]  # Shape: [batch_size, 2n]\n",
    "\n",
    "    # Compute second-order derivatives\n",
    "    J_H = torch.zeros(batch_size, 2*br_i, 2*br_i)  # Initialize Jacobian matrix\n",
    "\n",
    "    for i in range(2 * br_i):\n",
    "        if i<br_i:\n",
    "            grad_i = torch.autograd.grad(outputs=grad_h[:, br_i+i], inputs=y_tensor, grad_outputs=torch.ones_like(grad_h[:, br_i+i]), create_graph=True, retain_graph=True)[0]\n",
    "        else:\n",
    "            grad_i = torch.autograd.grad(outputs=-grad_h[:, i-br_i], inputs=y_tensor, grad_outputs=torch.ones_like(-grad_h[:, i-br_i]), create_graph=True, retain_graph=True)[0]\n",
    "        J_H[:, i, :] = grad_i  # Assign row-wise\n",
    "    \n",
    "    lam_tensor = lam.clone().detach().unsqueeze(2)\n",
    "\n",
    "\n",
    "    lam_dot = - (torch.bmm(J_H, lam_tensor)).squeeze()\n",
    "\n",
    "    return lam_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_gradients(flattened_gradients, original_shapes):\n",
    "    reshaped_gradients = []\n",
    "    start = 0\n",
    "    for shape in original_shapes:\n",
    "        size = torch.prod(torch.tensor(shape)).item()  # Calculate the number of elements in this shape\n",
    "        end = start + size\n",
    "        reshaped_gradients.append(flattened_gradients[start:end].reshape(shape))\n",
    "        start = end\n",
    "    return reshaped_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rk2_step(dyn, y, dt, dynamics, args, kargs):\n",
    "    h = dt\n",
    "    i = kargs[0]\n",
    "    q, p = y[:, 0], y[:, 1]\n",
    "\n",
    "    y = torch.stack((q, p), dim=-1)  # Shape: (batch_size, 2)\n",
    "\n",
    "    #print(q.shape)\n",
    "\n",
    "    dy1 = dynamics(y, args, kargs)\n",
    "    q1 = q + 0.5 * dy1[:, 0] * h\n",
    "    p1 = p + 0.5 * dy1[:, 1] * h\n",
    "\n",
    "    y1 = torch.stack((q1, p1), dim=-1)  # Shape: (batch_size, 2)\n",
    "    dy2 = dynamics(y1, args, kargs)\n",
    "\n",
    "    q_new = q + dy2[:, 0] * h\n",
    "    p_new = p + dy2[:, 1] * h\n",
    "    return torch.stack((q_new, p_new), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def im_step(y, dt, dynamics, iterations, y_init, args, kargs):\n",
    "#     h = dt\n",
    "#     br_i = y.shape[1] // 2\n",
    "#     q, p = y[:, 0:br_i], y[:, br_i:2 * br_i]\n",
    "    \n",
    "#     y_init_concat = torch.cat((y_init[:, 0:br_i], y_init[:, br_i:2*br_i]), dim=-1)  # Shape [batch, 2]\n",
    "#     f_init = dynamics(y_init_concat, args, kargs)  # Compute dynamics at initial point\n",
    "    \n",
    "#     q_new = q + 0.5 * h * f_init[:, 0:br_i]  # Shape [batch, 1]\n",
    "#     p_new = p + 0.5 * h * f_init[:, br_i:2*br_i]  # Shape [batch, 1]\n",
    "\n",
    "#     for _ in range(iterations):\n",
    "#         mid_q = 0.5 * (q + q_new)  # Shape [batch, q_shape]\n",
    "#         mid_p = 0.5 * (p + p_new)  # Shape [batch, p_shape]\n",
    "        \n",
    "#         mid_concat = torch.cat((mid_q, mid_p), dim=-1)  # Ensure [batch, 2*q_shape]\n",
    "#         f_mid = dynamics(mid_concat, args, kargs)  # Compute dynamics at midpoint\n",
    "        \n",
    "#         q_new = q + h * f_mid[:, 0:br_i]  # Shape [batch, q_shape]\n",
    "#         p_new = p + h * f_mid[:, br_i:2*br_i]  # Shape [batch, p_shape]\n",
    "\n",
    "#     return torch.cat((q_new, p_new), dim=-1)  # Final shape [batch, 2*q_shape]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sv_step(dyn, y, dt, dynamics, iterations, y_init, args, kargs):\n",
    "    h = dt\n",
    "    q, p = y[:, 0], y[:, 1]\n",
    "    i = kargs[0]\n",
    "\n",
    "    p_half = p + 0.5 * h * dynamics(torch.stack((q, y_init[:, 1]), dim=-1), args, kargs)[:, 1]\n",
    "    for _ in range(iterations):\n",
    "        p_half = p + 0.5 * h * dynamics(torch.stack((q, p_half), dim=-1), args, kargs)[:, 1]\n",
    "\n",
    "    q_half = q + 0.5 * h * dynamics(torch.stack((y_init[:, 0], p_half), dim=-1), args, kargs)[:, 0]\n",
    "    for _ in range(iterations):\n",
    "        q_half = q + 0.5 * h * dynamics(torch.stack((q_half, p), dim=-1), args, kargs)[:, 0]\n",
    "\n",
    "    q_new = q + h * dynamics(torch.stack((q_half, p_half), dim=-1), args, kargs)[:, 0]\n",
    "    p_new = p_half + 0.5 * h * dynamics(torch.stack((q_new, p_half), dim=-1), args, kargs)[:, 1]\n",
    "\n",
    "    return torch.stack((q_new, p_new), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_ivp_custom(dynamics, dyn, y0_batch, t_span, dt, args, iters):\n",
    "    #t = torch.arange(0, T, dt)\n",
    "    batch_size = y0_batch.shape[0]\n",
    "    t0, t1 = t_span\n",
    "    if t0 > t1:\n",
    "        dt = -dt\n",
    "    num_steps = int((t1 - t0) / dt) + 1\n",
    "    #y0_batch = noisy_obs[:, 0, :]\n",
    "    #ys_batch = torch.zeros(batch_size, num_steps, 2)\n",
    "    ys_batch = [y0_batch]\n",
    "    #print(y0_batch.shape)\n",
    "    #ys_batch[:, 0, :] = y0_batch.clone()\n",
    "\n",
    "\n",
    "    for i in range(1, num_steps):\n",
    "        #y = noisy_obs[:, i-1, :]  # Use the noisy observation at the current step\n",
    "        y = ys_batch[-1]\n",
    "        y_ = rk2_step(dyn, y, dt, dynamics, args, kargs=(i,))\n",
    "        y_next = sv_step(dyn, y, dt, dynamics, iters, y_, args, kargs=(i,))\n",
    "        #print(y_next.requires_grad)\n",
    "        ys_batch.append(y_next)\n",
    "        #print(y_next.shape)\n",
    "    ys_batch = torch.stack(ys_batch, dim=1)\n",
    "    #print(ys_batch.requires_grad)\n",
    "    return ys_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad(model, y_t, lambda_t, batch_size):\n",
    "\n",
    "    y_tensor = y_t.clone().detach().requires_grad_(True).to(y_t.device)\n",
    "    \n",
    "    # Perform forward pass\n",
    "    h = model(y_tensor)\n",
    "    \n",
    "    # Compute gradients of model output w.r.t y_tensor\n",
    "    grad_h = torch.autograd.grad(outputs=h.sum(), inputs=y_tensor,\n",
    "                                 create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "    \n",
    "    grad_w_p = torch.autograd.grad(outputs=grad_h[:, 1], inputs=model.parameters(), \n",
    "                                   grad_outputs=lambda_t[:, 0],\n",
    "                                   create_graph=True, retain_graph=True, allow_unused=True)\n",
    "    \n",
    "    grad_w_q = torch.autograd.grad(outputs=grad_h[:, 0], inputs=model.parameters(), \n",
    "                                   grad_outputs=lambda_t[:, 1],\n",
    "                                   create_graph=True, retain_graph=True, allow_unused=True)\n",
    "    \n",
    "    if grad_w_p is not None:\n",
    "        grad_w_p = torch.cat([p_grad.flatten() for p_grad in grad_w_p]).unsqueeze(0)\n",
    "        grad_w_p = grad_w_p.expand(batch_size, -1) / batch_size\n",
    "    \n",
    "    if grad_w_q is not None:\n",
    "        grad_w_q = torch.cat([p_grad.flatten() for p_grad in grad_w_q]).unsqueeze(0)\n",
    "        grad_w_q = grad_w_q.expand(batch_size, -1) / batch_size\n",
    "    \n",
    "    grad_w_combined = grad_w_p - grad_w_q\n",
    "\n",
    "    model.zero_grad()\n",
    "    \n",
    "    return grad_w_combined.mean(dim=0)  # avg over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(dynamics, dyn, lambdaT_batch, t_span, dt, args, iters):\n",
    "    #t = torch.arange(0, T, dt)\n",
    "    batch_size = lambdaT_batch.shape[0]\n",
    "    y_batch = args[1]\n",
    "    t0, t1 = t_span\n",
    "    model = args[0]\n",
    "    if t0 > t1:\n",
    "        dt = -dt\n",
    "    num_steps = int((t1 - t0) / dt) + 1\n",
    "    #y0_batch = noisy_obs[:, 0, :]\n",
    "    #ys_batch = torch.zeros(batch_size, num_steps, 2)\n",
    "    lambda_batch = lambdaT_batch\n",
    "    #print(y0_batch.shape)\n",
    "    #ys_batch[:, 0, :] = y0_batch.clone()\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    grad_result = torch.zeros(num_params, device=lambdaT_batch.device)\n",
    "\n",
    "    grad_result += (-dt / 2) * calculate_grad(model, y_batch[:,-1,:], lambdaT_batch, batch_size)\n",
    "    #print(\"Current grad: \", grad_result)\n",
    "\n",
    "\n",
    "    for i in range(1, num_steps):\n",
    "        #y = noisy_obs[:, i-1, :]  # Use the noisy observation at the current step\n",
    "        #y = ys_batch[-1]\n",
    "        lambda_ = rk2_step(dyn, lambda_batch, dt, dynamics, args, kargs=(i,))\n",
    "        lambda_next = sv_step(dyn, lambda_batch, dt, dynamics, iters, lambda_, args, kargs=(i,))\n",
    "        #print(y_next.requires_grad)\n",
    "        # Compute new gradient\n",
    "        current_grad = calculate_grad(model, y_batch[:,num_steps-i-1,:], lambda_next, batch_size)\n",
    "        #print(\"Current grad: \", current_grad)\n",
    "        \n",
    "        if i==num_steps-1:\n",
    "            grad_result += (-dt / 2) * (current_grad)\n",
    "        else:\n",
    "            grad_result += (-dt) * (current_grad)\n",
    "\n",
    "        lambda_batch = lambda_next\n",
    "        #print(y_next.shape)\n",
    "    #ys_batch = torch.stack(ys_batch, dim=1)\n",
    "    #print(ys_batch.requires_grad)\n",
    "    return grad_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_gt(gt_data, dt_solve, dt_gt):\n",
    "    downsample_factor = int(dt_solve / dt_gt)\n",
    "    return gt_data[:, ::downsample_factor, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_gt(gt_data, dt_solve, dt_gt):\n",
    "    downsample_factor = int(dt_solve / dt_gt)\n",
    "    return gt_data[:, ::downsample_factor, :]\n",
    "\n",
    "\n",
    "def load_data(datafolder, dynamics, dt_solve, dt_gt):\n",
    "\n",
    "    noisy_train_path = \"../data/\"+str(datafolder)+\"/noisy_\"+str(dynamics)+\"_train.pt\"\n",
    "    noisy_val_path = \"../data/\"+str(datafolder)+\"/noisy_\"+str(dynamics)+\"_val.pt\"\n",
    "    noisy_test_path = \"../data/\"+str(datafolder)+\"/noisy_\"+str(dynamics)+\"_test.pt\"\n",
    "\n",
    "    train_path = \"../data/\"+str(datafolder)+\"/\"+str(dynamics)+\"_train.pt\"\n",
    "    val_path = \"../data/\"+str(datafolder)+\"/\"+str(dynamics)+\"_val.pt\"\n",
    "    test_path = \"../data/\"+str(datafolder)+\"/\"+str(dynamics)+\"_test.pt\"\n",
    "\n",
    "    noisy_train_trajectories = torch.load(noisy_train_path).to(device)\n",
    "    noisy_val_trajectories = torch.load(noisy_val_path).to(device)\n",
    "\n",
    "    true_train_trajectories = torch.load(train_path).to(device)\n",
    "    true_val_trajectories = torch.load(val_path).to(device)\n",
    "\n",
    "\n",
    "    # Downsample ground truth data according to dt_solve\n",
    "    noisy_train_trajectories = downsample_gt(noisy_train_trajectories, dt_solve, dt_gt)\n",
    "    true_train_trajectories = downsample_gt(true_train_trajectories, dt_solve, dt_gt)\n",
    "\n",
    "\n",
    "    noisy_val_trajectories = downsample_gt(noisy_val_trajectories, dt_solve, dt_gt)\n",
    "    true_val_trajectories = downsample_gt(true_val_trajectories, dt_solve, dt_gt)\n",
    "\n",
    "\n",
    "    return noisy_train_trajectories, noisy_val_trajectories, true_train_trajectories, true_val_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(model, noisy_train_traj, noisy_val_traj, true_train_traj, true_val_traj, dt_gt, dt_solve, param_vals):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    num_epochs = 1\n",
    "\n",
    "    learning_rate = param_vals[\"lr\"]\n",
    "    \n",
    "    train_batch_size = param_vals[\"train_batch_size\"]\n",
    "    val_batch_size = param_vals[\"val_batch_size\"]\n",
    "    sim_len = param_vals[\"sim_len\"]\n",
    "    sims = param_vals[\"sims\"]\n",
    "    learning_rate = param_vals[\"lr\"]\n",
    "    train_batch_size = param_vals[\"train_batch_size\"]\n",
    "    val_batch_size = param_vals[\"val_batch_size\"]\n",
    "    T = param_vals[\"t_final\"]\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "\n",
    "    train_dataset = TensorDataset(noisy_train_traj, true_train_traj)\n",
    "    val_dataset = TensorDataset(noisy_val_traj, true_val_traj)\n",
    "\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    alpha = 1e-4\n",
    "    # Training loop\n",
    "    #print(\"Julian Time: \", )\n",
    "    print(f\"Params: Train size {noisy_train_traj.shape}, Val size {noisy_val_traj.shape}, Sim length {T} sec\")\n",
    "\n",
    "    #memory_usage_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        logging.info(f\"Progress: Step {epoch+1}\")\n",
    "\n",
    "        for batch in train_data_loader:\n",
    "            y_noisy_batch, y_true_batch = batch\n",
    "            y_noisy_batch = y_noisy_batch.to(device)\n",
    "            y_true_batch = y_true_batch.to(device)\n",
    "\n",
    "            ## Load the batch for the initial values of q and p\n",
    "            pq0_batch = torch.tensor(y_true_batch[:, 0, :], dtype=torch.float32)\n",
    "            \n",
    "            tracemalloc.start()\n",
    "            \n",
    "            ## Solve the forward ODE\n",
    "            y_pred_batch = solve_ivp_custom(forward_ode, \"forward\", pq0_batch, (0, T), dt_solve, args=(model,), iters=5)\n",
    "\n",
    "            y_pred_batch = y_pred_batch.requires_grad_(True)\n",
    "\n",
    "            loss = criterion(y_pred_batch[:, :, 0], y_noisy_batch[:,:,0]) + criterion(y_pred_batch[:, :, 1], y_noisy_batch[:,:,1])\n",
    "\n",
    "            lamb = torch.autograd.grad(loss, y_pred_batch, retain_graph=True)[0]\n",
    "            lamb_0 = lamb[:,-1,:]\n",
    "\n",
    "            y_pred_batch.detach()\n",
    "            # Track memory usage during backward pass\n",
    "\n",
    "            grads = backward(adjoint_ode, \"adjoint\", lamb_0, (T, 0), dt_solve, args=(model, y_pred_batch), iters=5)\n",
    "\n",
    "            _, peak_memory = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    " \n",
    "            #memory_usage_list.append({\"epoch\": epoch, \"peak_memory_MB\": peak_memory / (1024**2)})\n",
    "            #lambda_pred_batch = lambda_pred_batch.flip(1)\n",
    "\n",
    "            #grads = calculate_integral(model, y_pred_batch, T, lambda_pred_batch)\n",
    "\n",
    "            #Reshape the gradients to match the model parameters\n",
    "            start_idx = 0\n",
    "            for param in model.parameters():\n",
    "                param_shape = param.shape\n",
    "                param_size = param.numel()\n",
    "                param_grad = grads[start_idx:start_idx + param_size].reshape(param_shape)\n",
    "                param.grad = param_grad.clone().detach()\n",
    "                start_idx += param_size\n",
    "\n",
    "            # # Update the model parameters using the optimizer\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        average_train_loss = total_loss / (train_batch_size)\n",
    "        train_losses.append(average_train_loss)\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs}, Train Loss: {total_loss/len(train_data_loader)}')\n",
    "        scheduler.step(total_loss/len(train_data_loader))\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Log the time taken\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Objective function took {elapsed_time:.2f} seconds to complete\")\n",
    "    \n",
    "    return elapsed_time, peak_memory, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial:  0\n",
      "Params: Train size torch.Size([512, 4, 2]), Val size torch.Size([512, 4, 2]), Sim length 0.03 sec\n",
      "2025-03-18 23:00:40,592 - Progress: Step 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_256524/1319397858.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pq0_batch = torch.tensor(y_true_batch[:, 0, :], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Train Loss: 0.0038560437969863415\n",
      "Objective function took 0.56 seconds to complete\n",
      "Trial:  1\n",
      "Params: Train size torch.Size([512, 8, 2]), Val size torch.Size([512, 8, 2]), Sim length 0.07 sec\n",
      "2025-03-18 23:00:41,154 - Progress: Step 1\n",
      "Epoch 0/1, Train Loss: 0.018968800082802773\n",
      "Objective function took 1.16 seconds to complete\n",
      "Trial:  2\n",
      "Params: Train size torch.Size([512, 12, 2]), Val size torch.Size([512, 12, 2]), Sim length 0.11 sec\n",
      "2025-03-18 23:00:42,330 - Progress: Step 1\n",
      "Epoch 0/1, Train Loss: 0.045060932636260986\n",
      "Objective function took 1.80 seconds to complete\n",
      "Trial:  3\n",
      "Params: Train size torch.Size([512, 16, 2]), Val size torch.Size([512, 16, 2]), Sim length 0.15 sec\n",
      "2025-03-18 23:00:44,160 - Progress: Step 1\n",
      "Epoch 0/1, Train Loss: 0.08154118806123734\n",
      "Objective function took 2.51 seconds to complete\n",
      "Trial:  4\n",
      "Params: Train size torch.Size([512, 20, 2]), Val size torch.Size([512, 20, 2]), Sim length 0.19 sec\n",
      "2025-03-18 23:00:46,718 - Progress: Step 1\n",
      "Epoch 0/1, Train Loss: 0.1273912489414215\n",
      "Objective function took 2.96 seconds to complete\n",
      "Memory usage data saved to memory_usage_results.csv\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "    # # Parse command line arguments\n",
    "    # parser = argparse.ArgumentParser(description=\"Enter the simulation parameters\")\n",
    "    # parser.add_argument(\"--dynamics_name\", type=str, required=True, choices=[\"mass_spring\", \"double_well\", \"coupled_ho\", \"henon_heiles\"], help=\"The name of the dynamics function.\")\n",
    "    # parser.add_argument(\"--data_folder\", type=str, required=True, help=\"the ground truth data folder\")\n",
    "    # parser.add_argument(\"--gt_res\", type=float, required=True, help=\"the ground truth resolution/stepsize\")\n",
    "    # parser.add_argument(\"--hid_layers\", type=parse_hidden_layers, required=True,\n",
    "    #                     help=\"Hidden layers as a list of integers, e.g., [16,32,16]\")\n",
    "    # parser.add_argument(\"--solver_res\", type=float, required=True, help=\"The time step length for our solver(= k*gt_res where k is an integer)\")\n",
    "    # parser.add_argument(\"--noise_level\", type=float, required=False, default=0.0,\n",
    "    #                 help=\"The noise level (a float number from data_gen). Default is 0.0.\")\n",
    "    # parser.add_argument(\"--pred\", type=lambda x: bool(distutils.util.strtobool(x)), required=False, default=False, \n",
    "    #                 help=\"Boolean flag: True if you need a predictor step, False if you use GT (default: False)\")\n",
    "    # parser.add_argument(\"--num_sims\", type=int, required=False, default=1, help=\"The number of multi-shooting trajectories (default= 1 single shooting)\")\n",
    "    # parser.add_argument(\"--sim_len\", type=int, required=True, help=\"The forward simulation length of each trajectory for training.\")\n",
    "    # parser.add_argument(\"--solver\", type=str, required=False, default=\"im\", choices=[\"im\",\"sv\"])\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    \n",
    "dynamics_name = \"mass_spring\"\n",
    "data_folder = \"mass_spring_10\"\n",
    "dt_gt = 0.01\n",
    "hidden_layer_sizes = [16,32,16]\n",
    "dt_solve = 0.01\n",
    "noise_level = 0\n",
    "pred = True\n",
    "num_sims = 1\n",
    "sim_len = 7\n",
    "solver = \"im\"\n",
    "\n",
    "noisy_train, noisy_val, true_train, true_val = load_data(data_folder, dynamics_name, dt_gt, dt_solve)\n",
    "\n",
    "input_size = noisy_train.shape[2]\n",
    "output_size = 1\n",
    "\n",
    "train_set_len = int(noisy_train.shape[0])\n",
    "val_set_len = int(noisy_val.shape[0])\n",
    "\n",
    "layer_sizes = [input_size] + hidden_layer_sizes + [output_size]\n",
    "\n",
    "model_specs = (layer_sizes,)\n",
    "\n",
    "model = HamiltonianNN(model_specs).to(device) \n",
    "\n",
    "params_list = [{\"sim_len\":4, \"lr\":0.01, \"pred\":pred, \"solver\":solver,\n",
    "                    \"train_batch_size\":512, \"val_batch_size\":512, \"t_final\":0.03, \"sims\": num_sims}\n",
    "                ,{\"sim_len\":8, \"lr\":0.01, \"pred\":pred, \"solver\":solver,\n",
    "                    \"train_batch_size\":512, \"val_batch_size\":512, \"t_final\":0.07, \"sims\": num_sims}\n",
    "                  ,{\"sim_len\":12, \"lr\":0.01, \"pred\":pred, \"solver\":solver,\n",
    "                    \"train_batch_size\":512, \"val_batch_size\":512, \"t_final\":0.11, \"sims\": num_sims},\n",
    "                    {\"sim_len\":16, \"lr\":0.01, \"pred\":pred, \"solver\":solver,\n",
    "                    \"train_batch_size\":512, \"val_batch_size\":512, \"t_final\":0.15, \"sims\": num_sims},\n",
    "                    {\"sim_len\":20, \"lr\":0.01, \"pred\":pred, \"solver\":solver,\n",
    "                    \"train_batch_size\":512, \"val_batch_size\":512, \"t_final\":0.19, \"sims\": num_sims},\n",
    "                    {\"sim_len\":24, \"lr\":0.01, \"pred\":pred, \"solver\":solver,\n",
    "                    \"train_batch_size\":512, \"val_batch_size\":512, \"t_final\":0.23, \"sims\": num_sims},\n",
    "                    {\"sim_len\":28, \"lr\":0.01, \"pred\":pred, \"solver\":solver,\n",
    "                    \"train_batch_size\":512, \"val_batch_size\":512, \"t_final\":0.27, \"sims\": num_sims},\n",
    "                    {\"sim_len\":32, \"lr\":0.01, \"pred\":pred, \"solver\":solver,\n",
    "                    \"train_batch_size\":512, \"val_batch_size\":512, \"t_final\":0.31, \"sims\": num_sims}\n",
    "                    ]\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "models = []\n",
    "memory_usage_list = []\n",
    "time_list = []\n",
    "\n",
    "for i in range(len(params_list)):\n",
    "    \n",
    "    #start_ind = int(params_list[i][\"t_start\"]/dt_solve)\n",
    "    end_ind = params_list[i][\"sim_len\"]\n",
    "    train_batch_size = params_list[i][\"train_batch_size\"]\n",
    "    val_batch_size = params_list[i][\"val_batch_size\"]\n",
    "\n",
    "\n",
    "    print(\"Trial: \", str(i))\n",
    "    \n",
    "    elapsed_time, peak_memory, model = objective(model, noisy_train[0:train_batch_size, 0:end_ind, :],\n",
    "                                                noisy_val[0:val_batch_size, 0:end_ind, :], \n",
    "                                                true_train[0:train_batch_size, 0:end_ind, :], \n",
    "                                                true_val[0:val_batch_size, 0:end_ind, :], \n",
    "                                                dt_gt, dt_solve, params_list[i])\n",
    "    \n",
    "    memory_usage_list.append({\"peak_memory_MB\": peak_memory / (1024**2)})\n",
    "    time_list.append({\"run_time\":elapsed_time})\n",
    "\n",
    "\n",
    "# Save memory tracking results\n",
    "df = pd.DataFrame(zip(memory_usage_list, time_list))\n",
    "df.to_csv(\"adjoint_memory_usage_results_2.csv\", index=False)\n",
    "print(\"Memory usage data saved to memory_usage_results.csv\")\n",
    "\n",
    "\n",
    "    #torch.save(model, f'../models/model_{i}_{dynamics_name}_{noise_level}_adjoint_{num_sims}_{sim_len}_{solver}.pt')\n",
    "    # df = pd.DataFrame({\n",
    "    # \"train_loss\": train_loss,\n",
    "    # \"val_loss\": val_loss\n",
    "    # })\n",
    "    # df.to_csv(f'output_{dynamics_name}_{noise_level}_adjoint_{num_sims}_{sim_len}_{solver}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ghost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
